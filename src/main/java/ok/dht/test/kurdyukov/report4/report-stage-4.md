# Отчет

## Введение

С прошлого этапа были удалены листенеры, которые обеспечивали circuit breaker.
По двум причинам:

1. Этот подход не будет работать на большом кластере. Ибо мы неспособны создать тысячи тредов, которые будут слушать
   каждый узел.
2. Также для того, чтобы код стал более читаемый.
3. Экономия ресурсов локальной машинки. Ибо потоки нужны и для wrk2, а также для кластера из 3 нод.

Также был сильно изменена структура проекта:

1. Был сделан класс для работы с базой была вынесена в `DaoRepository`. Здесь мы никак не взаимодействуем с `one-nio`.
2. `DaoEntry` класс обертка над данными, которые в итоге хранятся в сериализованном виде. Это timestamp, value,
   isTombstone.
3. Классы, которые взаимодействуют с http теперь лежат в пакете `http`.
4. Также появился `ObjectMapper`, который сериализует и десериализует DaoEntry.

Также с преподавательским составом утвердилось то, что мы доверяем пользователю. Например, он не может что - то положить
в хедер. Также общение внутри кластера также доверительное.

Считаем как будто перед нами стоит nginx.

## Описание работы кластера

Обработка пользовательских запросов имеет два четких этапа (и неважно, какая нода стала обрабатывать запрос
пользователя):

1. Multicast размера `from`. Возможно сообщение будет отправлено само себе. Опять же для удобства, сообщения
   рассылаются асинхронно.
2. Блокируемся и дожидаемся ответов, делаем мерж результатов.

При удалении происходит пут пустого массива байт с флажком "могилки".

Политика реплицирования была выбрана следующая. Все ноды стартуют с одним и тем же порядоком урлов кластера.
С помощью консистетного хеширования мы находим ноду куда нужно реплицироваться. Затем берем следующие from - 1 узлов и
на них реплицируем.

## Ход работы

При нагрузочном тестировании использовался кластер из трех нод. Параметры для реплицирования были следующими ack = 2 и
from = 3.